{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model1_provaPrecisione.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FG2511/ARE/blob/master/model1_provaPrecisione.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "hu-_5aP7LlL6",
        "colab_type": "code",
        "outputId": "a939950a-3fae-41a2-a59f-16b312d1d35a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "@File name: model1.ipynb\n",
        "@Created on 2018-12-20\n",
        "@Authors: Federica Gerina, Francesca Moi, Silvia Maria Massa\n",
        "@Description: Given a time-series dataset that contains minute-by-minute data \n",
        "about different kind of gases, collected by the uHoo air quality sensor, train\n",
        "a NN that classifies if a minute belongs to the class \"Pasto\" (1) otherwise to\n",
        "the class \"Other\" (0).\n",
        "'''\n",
        "\n",
        "!pip install liac-arff\n",
        "\n",
        "import arff\n",
        "import numpy as np\n",
        "\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from keras.layers import Dense, Dropout, LeakyReLU, BatchNormalization, Activation\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from sklearn.utils import compute_class_weight\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import sys\n",
        "sys.path.append('local_modules')\n",
        "\n",
        "import postprocessing_sw"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: liac-arff in /usr/local/lib/python3.6/dist-packages (2.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DJ0SubdGw6LA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#fix random seed for reproducibility\n",
        "seed = 5\n",
        "np.random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yKvmDiR9NgBO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "@Description: generate a multilayer perceptron with LeakyRelu as activation\n",
        "function.\n",
        "@param: \n",
        "  - shape : int, the shape of the input\n",
        "  - n_features: int, the number of features given\n",
        "'''\n",
        "\n",
        "def generate_model_leaky(shape, n_features):\n",
        "\n",
        "  units_1 = int(n_features/2)\n",
        "  units_2 = n_features\n",
        "  units_3 = n_features*2\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(BatchNormalization())\n",
        "  \n",
        "  model.add(Dense(units_1, input_dim=shape, kernel_initializer='random_uniform',  use_bias = False))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU(alpha = 0.2))\n",
        "  model.add(Dropout(0.5))\n",
        "  \n",
        "  model.add(Dense(units_2, kernel_initializer='random_uniform',  use_bias = False))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU(alpha = 0.2))\n",
        "  model.add(Dropout(0.5))\n",
        "  \n",
        "  model.add(Dense(units_3, kernel_initializer='random_uniform',  use_bias = False))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU(alpha = 0.2))\n",
        "  model.add(Dropout(0.5))\n",
        "  \n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  #print(model.summary())\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "o99ibbgGHANE",
        "outputId": "91c4278b-a06d-4f6e-a8b3-220864c895f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1428
        }
      },
      "cell_type": "code",
      "source": [
        "#@title CHOOSE\n",
        "\n",
        "'''\n",
        "@Description: MAIN\n",
        "'''\n",
        "\n",
        "#LOAD DATA\n",
        "print(\"Loading data...\")\n",
        "\n",
        "dataset = '/root/data/uHooComplete_featureDataset_Past_Only.arff' #@param {type:\"string\"}\n",
        "\n",
        "with open (dataset, encoding='utf-8') as f:\n",
        "  dataDictionary = arff.load(f)\n",
        "\n",
        "data = np.array(dataDictionary['data'])\n",
        "print(\"DATASET LOADED\")\n",
        "\n",
        "#CONVERTING VALUES\n",
        "print(\"\\nConverting values...\")\n",
        "for i in data:\n",
        "  if(i[-1] == 'Other'): i[-1] = 0\n",
        "  elif(i[-1] == 'Pasto') : i[-1] = 1\n",
        "\n",
        "dataset = data.astype('float32')\n",
        "print(\"CONVERSION DONE\")\n",
        "\n",
        "#SPLIT INTO INPUT (X) AND OUTPUT (Y) VARIABLES\n",
        "s = dataset.shape[-1]\n",
        "#print(s)\n",
        "X = dataset[:,0:s-1]\n",
        "Y = dataset[:,s-1]\n",
        "\n",
        "#print(s-1)\n",
        "\n",
        "n_features = s-1\n",
        "\n",
        "#SPLIT INTO TRAINING, VALIDATION AND TEST SETS\n",
        "print(\"\\nSplit into training, validation and test sets...\")\n",
        "\n",
        "train_rate = 80\n",
        "val_rate = 10\n",
        "train = round(int((dataset.shape[0]*train_rate)/100))\n",
        "val = round(int((dataset.shape[0]*(train_rate+val_rate))/100))\n",
        "\n",
        "train_data = X[:train]\n",
        "train_label = Y[:train]\n",
        "\n",
        "val_data = X[train+1:val]\n",
        "val_label = Y[train+1:val]\n",
        "\n",
        "test_data = X[val+1:]\n",
        "test_label = Y[val+1:]\n",
        "print(\"DATASET SPLITTED\")\n",
        "\n",
        "#COMPUTE CLASS WEIGHT\n",
        "labels = np.unique(train_label)\n",
        "classWeight = compute_class_weight('balanced', labels, train_label)\n",
        "classWeight = dict(zip(labels,classWeight))\n",
        "\n",
        "#GENERATE MODEL\n",
        "print(\"\\nGenerate model...\")\n",
        "\n",
        "model = generate_model_leaky(train_data.shape[-1], n_features)\n",
        "\n",
        "#OPTIMIZERS\n",
        "adm = optimizers.Adam(lr=0.0001)\n",
        "\n",
        "#COMPILE MODEL\n",
        "print(\"\\nCompile model...\")\n",
        "model.compile(loss='binary_crossentropy', optimizer = adm , metrics=['accuracy'])\n",
        "\n",
        "#EARLY STOPPING\n",
        "es = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
        "\n",
        "#FIT MODEL\n",
        "print(\"\\nFit model...\")\n",
        "history = model.fit(train_data, train_label, epochs=50, validation_data = (val_data, val_label), batch_size = 128, shuffle = True, class_weight = classWeight, verbose=1, callbacks = [es])\n",
        "\n",
        "#EVALUATE MODEL\n",
        "print(\"\\nEvaluate model...\")\n",
        "scores_test = model.evaluate(test_data, test_label, batch_size=128, verbose = 1)\n",
        "print(\"Test loss: %.2f%%\" % (scores_test[0] * 100))\n",
        "print(\"Test accuracy: %.2f%%\" % (scores_test[1] * 100))\n",
        "\n",
        "#CALCULATE PREDICTIONS\n",
        "print(\"\\nCalculate predictions...\")\n",
        "pred = model.predict_classes(test_data, batch_size=128, verbose=0)\n",
        "flat_pred = [item for sublist in pred for item in sublist]\n",
        "\n",
        "#CONFUSION MATRIX BEFORE POST PROCESSING\n",
        "print(\"\\nCompute confusion matrix BEFORE POST PROCESSING...\")\n",
        "y_true = test_label\n",
        "y_pred = pred\n",
        "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "print(\"TN\", tn)\n",
        "print(\"FP\", fp)\n",
        "print(\"FN\", fn)\n",
        "print(\"TP\", tp)\n",
        "other = 100*tn/(tn+fp)\n",
        "pasto = 100*tp/(fn+tp)\n",
        "precision = 100*(tp/(tp+fp))\n",
        "print(\"Other corretti: %.2f %%\" % other)\n",
        "print(\"Pasto corretti: %.2f %%\" % pasto)\n",
        "print(\"Precisione: %.2f %%\" % precision)\n",
        "\n",
        "time = []\n",
        "for i in test_data:\n",
        "  time.append(i[-5])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "DATASET LOADED\n",
            "\n",
            "Converting values...\n",
            "CONVERSION DONE\n",
            "\n",
            "Split into training, validation and test sets...\n",
            "DATASET SPLITTED\n",
            "\n",
            "Generate model...\n",
            "\n",
            "Compile model...\n",
            "\n",
            "Fit model...\n",
            "Train on 280392 samples, validate on 35048 samples\n",
            "Epoch 1/50\n",
            "280392/280392 [==============================] - 11s 38us/step - loss: 0.6437 - acc: 0.6834 - val_loss: 0.7878 - val_acc: 0.4497\n",
            "Epoch 2/50\n",
            "280392/280392 [==============================] - 9s 32us/step - loss: 0.5480 - acc: 0.7413 - val_loss: 0.7568 - val_acc: 0.4513\n",
            "Epoch 3/50\n",
            "280392/280392 [==============================] - 9s 32us/step - loss: 0.5156 - acc: 0.7505 - val_loss: 0.7439 - val_acc: 0.4535\n",
            "Epoch 4/50\n",
            "280392/280392 [==============================] - 9s 31us/step - loss: 0.4993 - acc: 0.7521 - val_loss: 0.7017 - val_acc: 0.5439\n",
            "Epoch 5/50\n",
            "280392/280392 [==============================] - 9s 31us/step - loss: 0.4922 - acc: 0.7583 - val_loss: 0.7094 - val_acc: 0.5260\n",
            "Epoch 6/50\n",
            "280392/280392 [==============================] - 9s 31us/step - loss: 0.4813 - acc: 0.7614 - val_loss: 0.6809 - val_acc: 0.5706\n",
            "Epoch 7/50\n",
            "280392/280392 [==============================] - 9s 31us/step - loss: 0.4767 - acc: 0.7636 - val_loss: 0.6418 - val_acc: 0.6227\n",
            "Epoch 8/50\n",
            "280392/280392 [==============================] - 9s 31us/step - loss: 0.4734 - acc: 0.7644 - val_loss: 0.5896 - val_acc: 0.6652\n",
            "Epoch 9/50\n",
            "280392/280392 [==============================] - 9s 31us/step - loss: 0.4682 - acc: 0.7649 - val_loss: 0.5765 - val_acc: 0.6760\n",
            "Epoch 10/50\n",
            "280392/280392 [==============================] - 9s 32us/step - loss: 0.4630 - acc: 0.7682 - val_loss: 0.5462 - val_acc: 0.6943\n",
            "Epoch 11/50\n",
            "280392/280392 [==============================] - 9s 32us/step - loss: 0.4614 - acc: 0.7685 - val_loss: 0.5385 - val_acc: 0.6959\n",
            "Epoch 12/50\n",
            "280392/280392 [==============================] - 9s 32us/step - loss: 0.4589 - acc: 0.7690 - val_loss: 0.5234 - val_acc: 0.7031\n",
            "Epoch 13/50\n",
            "280392/280392 [==============================] - 9s 31us/step - loss: 0.4571 - acc: 0.7689 - val_loss: 0.5281 - val_acc: 0.7035\n",
            "Epoch 14/50\n",
            "280392/280392 [==============================] - 9s 33us/step - loss: 0.4549 - acc: 0.7707 - val_loss: 0.5147 - val_acc: 0.7029\n",
            "Epoch 15/50\n",
            "280392/280392 [==============================] - 9s 33us/step - loss: 0.4518 - acc: 0.7721 - val_loss: 0.5049 - val_acc: 0.7253\n",
            "Epoch 16/50\n",
            "280392/280392 [==============================] - 9s 33us/step - loss: 0.4514 - acc: 0.7683 - val_loss: 0.4945 - val_acc: 0.7394\n",
            "Epoch 17/50\n",
            "280392/280392 [==============================] - 9s 31us/step - loss: 0.4506 - acc: 0.7705 - val_loss: 0.5037 - val_acc: 0.7218\n",
            "Epoch 18/50\n",
            "280392/280392 [==============================] - 9s 32us/step - loss: 0.4496 - acc: 0.7731 - val_loss: 0.4893 - val_acc: 0.7526\n",
            "Epoch 19/50\n",
            "280392/280392 [==============================] - 11s 38us/step - loss: 0.4492 - acc: 0.7715 - val_loss: 0.4874 - val_acc: 0.7393\n",
            "Epoch 20/50\n",
            "280392/280392 [==============================] - 9s 33us/step - loss: 0.4461 - acc: 0.7720 - val_loss: 0.4930 - val_acc: 0.7376\n",
            "Epoch 21/50\n",
            "280392/280392 [==============================] - 9s 33us/step - loss: 0.4472 - acc: 0.7734 - val_loss: 0.4713 - val_acc: 0.7768\n",
            "Epoch 22/50\n",
            "280392/280392 [==============================] - 9s 33us/step - loss: 0.4462 - acc: 0.7722 - val_loss: 0.4679 - val_acc: 0.7703\n",
            "Epoch 23/50\n",
            "280392/280392 [==============================] - 9s 33us/step - loss: 0.4447 - acc: 0.7722 - val_loss: 0.4773 - val_acc: 0.7704\n",
            "Epoch 24/50\n",
            "280392/280392 [==============================] - 10s 34us/step - loss: 0.4437 - acc: 0.7787 - val_loss: 0.4558 - val_acc: 0.7986\n",
            "Epoch 25/50\n",
            "280392/280392 [==============================] - 9s 31us/step - loss: 0.4429 - acc: 0.7788 - val_loss: 0.4710 - val_acc: 0.7653\n",
            "Epoch 26/50\n",
            "280392/280392 [==============================] - 9s 33us/step - loss: 0.4395 - acc: 0.7781 - val_loss: 0.4828 - val_acc: 0.7450\n",
            "\n",
            "Evaluate model...\n",
            "35049/35049 [==============================] - 0s 9us/step\n",
            "Test loss: 53.81%\n",
            "Test accuracy: 70.32%\n",
            "\n",
            "Calculate predictions...\n",
            "\n",
            "Compute confusion matrix BEFORE POST PROCESSING...\n",
            "TN 22971\n",
            "FP 10010\n",
            "FN 391\n",
            "TP 1677\n",
            "Other corretti: 69.65 %\n",
            "Pasto corretti: 81.09 %\n",
            "Precisione: 14.35 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hdkk1t9J6oTb",
        "colab_type": "code",
        "outputId": "afb6df0c-677e-48b7-b9fb-7536e1a64926",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "new_pred = postprocessing_sw.sliding_windows(flat_pred,45)\n",
        "\n",
        "#CONFUSION MATRIX AFTER POST PROCESSING\n",
        "print(\"\\nCompute NEW confusion matrix AFTER POST PROCESSING...\")\n",
        "y_true = test_label\n",
        "tn, fp, fn, tp = confusion_matrix(y_true, new_pred).ravel()\n",
        "print(\"TN\", tn)\n",
        "print(\"FP\", fp)\n",
        "print(\"FN\", fn)\n",
        "print(\"TP\", tp)\n",
        "other = 100*tn/(tn+fp)\n",
        "pasto = 100*tp/(fn+tp)\n",
        "precision = 100*(tp/(tp+fp))\n",
        "print(\"Other corretti: %.2f %%\" % other)\n",
        "print(\"Pasto corretti: %.2f %%\" % pasto)\n",
        "print(\"Precisione: %.2f %%\" % precision)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "SLIDING WINDOWS FUNCTION...\n",
            "\n",
            "Compute NEW confusion matrix AFTER POST PROCESSING...\n",
            "TN 22961\n",
            "FP 10020\n",
            "FN 361\n",
            "TP 1707\n",
            "Other corretti: 69.62 %\n",
            "Pasto corretti: 82.54 %\n",
            "Precisione: 14.56 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A4lyMbqy0rWL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_precision(p, r):\n",
        "  index_pred = []\n",
        "  index_real = []\n",
        "\n",
        "  prediction_meals = []\n",
        "  real_meals = []\n",
        "\n",
        "  #p = flat_pred\n",
        "  #r = test_label\n",
        "\n",
        "  #Costruzione delle liste di indici: una di pasti predetti e una di pasti reali\n",
        "\n",
        "  for i in range(0, len(p)):\n",
        "    if p[i] == 1:\n",
        "      index_pred.append(i)\n",
        "      if ((i+1) in range(0, len(p)) and p[i+1] == 0) or (i+1) not in range(0, len(p)): \n",
        "        if(len(index_pred)>5):        \n",
        "          prediction_meals.append(index_pred)\n",
        "        index_pred = []\n",
        "    if r[i] == 1:\n",
        "      index_real.append(i)\n",
        "      if ((i+1) in range(0, len(p)) and r[i+1] == 0) or (i+1) not in range(0, len(p)): \n",
        "        real_meals.append(index_real)\n",
        "        index_real = []\n",
        "\n",
        "  #Ricerca delle intersezioni\n",
        "\n",
        "  intersection = []\n",
        "  tp = 0\n",
        "  fp = 0\n",
        "\n",
        "  for i in range(0, len(real_meals)): #inizializzazione\n",
        "    intersection.append(0)\n",
        "\n",
        "  for i in range(0, len(prediction_meals)): #per ogni pasto predetto\n",
        "    flag_found = 0 #per tenere conto se l'intersezione è già stata trovata all'interno del pasto reale\n",
        "\n",
        "    j=0\n",
        "    while j in range(0, len(real_meals)) and flag_found==0: # per ogni pasto reale    \n",
        "      #se almeno un elemento di prediction_meals[i] è presente in j\n",
        "      flag_visited = 0 \n",
        "\n",
        "      for x in prediction_meals[i]: #da qui potrei mettere una funz\n",
        "\n",
        "        if x in real_meals[j]: #se c'è l'intersezione        \n",
        "\n",
        "          if intersection[j]==0: #controlla che non sia già stata trovata          \n",
        "            intersection[j] = 1\n",
        "            flag_visited = 1 #imposta il flag a uno per indicare che il vero positivo è stato già individuato e per non contarlo più di una volta\n",
        "            flag_found = 1 #imposta il flag a uno per indicare che il pasto predetto è stato trovato nei pasti reali\n",
        "            tp=tp+1\n",
        "\n",
        "          elif flag_visited == 0: #se è stata già trovata per un altro pasto e quindi è la prima volta che controllo questo pasto predetto,\n",
        "            fp=fp+1\n",
        "            flag_visited = 1 #imposta il flag a uno per indicare che il falso positivo è stato già individuato e per non contarlo più di una volta\n",
        "            flag_found = 1\n",
        "\n",
        "      j = j + 1  \n",
        "    #end for j\n",
        "\n",
        "    if flag_found == 0: #se l'intersezione nn è stata trovata nei pasti reali il pasto predetto è falso\n",
        "      fp=fp+1\n",
        "      #print(\"FP3\")\n",
        "\n",
        "  #Calcolo accuratezza\n",
        "  print(\"\\nPrecision: \")\n",
        "  print(\"N° pasti reali:\", len(real_meals))\n",
        "  print(\"N° pasti predetti:\", len(prediction_meals))\n",
        "\n",
        "  print(\"TP:\", tp)\n",
        "  print(\"FP:\", fp)\n",
        "\n",
        "  somma = tp+fp\n",
        "  precision = (tp/somma)*100\n",
        "  TPR = (tp/len(real_meals))*100\n",
        "\n",
        "  print(\"Tot = \", somma)\n",
        "  print(\"TPR = \", TPR)\n",
        "  print(\"Precisione: %.2f %%\" % precision)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EHk6Ho1b1ZiQ",
        "colab_type": "code",
        "outputId": "a71ce09e-ebfb-43a4-cf1e-d6f621b82dc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "get_precision(flat_pred,test_label)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Precision: \n",
            "N° pasti reali: 82\n",
            "N° pasti predetti: 307\n",
            "TP: 70\n",
            "FP: 237\n",
            "Tot =  307\n",
            "TPR =  85.36585365853658\n",
            "Precisione: 22.80 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CVJeOMxJ2kTL",
        "colab_type": "code",
        "outputId": "928ebfa7-f9fb-4464-8c22-9299aec24eba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "get_precision(new_pred,test_label)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Precision: \n",
            "N° pasti reali: 82\n",
            "N° pasti predetti: 114\n",
            "TP: 60\n",
            "FP: 54\n",
            "Tot =  114\n",
            "TPR =  73.17073170731707\n",
            "Precisione: 52.63 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MbZuKtBit8QX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6439d27b-7899-4f20-d805-62173031ec5f"
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "def getMeals(pred, real):\n",
        "  index=[]\n",
        "  index_pred = []\n",
        "  index_real = []\n",
        "\n",
        "  prediction_meals = []\n",
        "  real_meals = []\n",
        "\n",
        "  #Costruzione delle liste: una di pasti predetti e una di pasti reali\n",
        "\n",
        "  #Costruzione pasti predetti\n",
        "\n",
        "  for i in range(0, len(pred)):\n",
        "\n",
        "    if pred[i] == 0:  \n",
        "\n",
        "      index.append(i)\n",
        "\n",
        "      if ((i+1) in range(0, len(pred)) and pred[i+1] == 1) or (i+1) not in range(0, len(pred)): \n",
        "\n",
        "        index_pred.append(index)\n",
        "        prediction_meals.append(0)\n",
        "        index = []\n",
        "\n",
        "    if pred[i] == 1:\n",
        "\n",
        "      index.append(i)\n",
        "\n",
        "      if ((i+1) in range(0, len(pred)) and pred[i+1] == 0) or (i+1) not in range(0, len(pred)): \n",
        "\n",
        "        index_pred.append(index)\n",
        "        prediction_meals.append(1)\n",
        "        index = []\n",
        "\n",
        "  #Costruzione pasti reali  \n",
        "\n",
        "  for i in range(0, len(real)):\n",
        "\n",
        "    if real[i] == 0:  \n",
        "\n",
        "      index.append(i)\n",
        "\n",
        "      if ((i+1) in range(0, len(real)) and real[i+1] == 1) or (i+1) not in range(0, len(real)): \n",
        "\n",
        "        index_real.append(index)\n",
        "        real_meals.append(0)\n",
        "        index = []\n",
        "\n",
        "    if real[i] == 1:\n",
        "\n",
        "      index.append(i)\n",
        "\n",
        "      if ((i+1) in range(0, len(real)) and real[i+1] == 0) or (i+1) not in range(0, len(real)): \n",
        "\n",
        "        index_real.append(index)\n",
        "        real_meals.append(1)\n",
        "        index = []\n",
        "\n",
        "  return prediction_meals\n",
        "'''"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef getMeals(pred, real):\\n  index=[]\\n  index_pred = []\\n  index_real = []\\n\\n  prediction_meals = []\\n  real_meals = []\\n\\n  #Costruzione delle liste: una di pasti predetti e una di pasti reali\\n\\n  #Costruzione pasti predetti\\n\\n  for i in range(0, len(pred)):\\n\\n    if pred[i] == 0:  \\n\\n      index.append(i)\\n\\n      if ((i+1) in range(0, len(pred)) and pred[i+1] == 1) or (i+1) not in range(0, len(pred)): \\n\\n        index_pred.append(index)\\n        prediction_meals.append(0)\\n        index = []\\n\\n    if pred[i] == 1:\\n\\n      index.append(i)\\n\\n      if ((i+1) in range(0, len(pred)) and pred[i+1] == 0) or (i+1) not in range(0, len(pred)): \\n\\n        index_pred.append(index)\\n        prediction_meals.append(1)\\n        index = []\\n\\n  #Costruzione pasti reali  \\n\\n  for i in range(0, len(real)):\\n\\n    if real[i] == 0:  \\n\\n      index.append(i)\\n\\n      if ((i+1) in range(0, len(real)) and real[i+1] == 1) or (i+1) not in range(0, len(real)): \\n\\n        index_real.append(index)\\n        real_meals.append(0)\\n        index = []\\n\\n    if real[i] == 1:\\n\\n      index.append(i)\\n\\n      if ((i+1) in range(0, len(real)) and real[i+1] == 0) or (i+1) not in range(0, len(real)): \\n\\n        index_real.append(index)\\n        real_meals.append(1)\\n        index = []\\n\\n  return prediction_meals\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "Mt855zXut9fi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#real_meals = getMeals(flat_pred, test_label)\n",
        "#print(len(real_meals))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}